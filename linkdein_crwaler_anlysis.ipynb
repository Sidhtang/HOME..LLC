{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sidhtang/HOME..LLC/blob/main/linkdein_crwaler_anlysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install linkedin-api"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_CVnHp1GVeKi",
        "outputId": "bd231c12-acde-4e72-c61c-9305b4e3a2aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting linkedin-api\n",
            "  Downloading linkedin_api-2.3.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.10/dist-packages (from linkedin-api) (4.12.3)\n",
            "Collecting lxml<6.0.0,>=5.3.0 (from linkedin-api)\n",
            "  Downloading lxml-5.3.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.3 in /usr/local/lib/python3.10/dist-packages (from linkedin-api) (2.32.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->linkedin-api) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.32.3->linkedin-api) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.32.3->linkedin-api) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.32.3->linkedin-api) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.32.3->linkedin-api) (2024.8.30)\n",
            "Downloading linkedin_api-2.3.0-py3-none-any.whl (26 kB)\n",
            "Downloading lxml-5.3.0-cp310-cp310-manylinux_2_28_x86_64.whl (5.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lxml, linkedin-api\n",
            "  Attempting uninstall: lxml\n",
            "    Found existing installation: lxml 4.9.4\n",
            "    Uninstalling lxml-4.9.4:\n",
            "      Successfully uninstalled lxml-4.9.4\n",
            "Successfully installed linkedin-api-2.3.0 lxml-5.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import time\n",
        "import random\n",
        "from typing import List, Dict, Any\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "class LinkedInScraper:\n",
        "    def __init__(self, email: str, password: str):\n",
        "        \"\"\"\n",
        "        Initialize LinkedIn scraper with login credentials\n",
        "        Args:\n",
        "            email (str): LinkedIn login email\n",
        "            password (str): LinkedIn login password\n",
        "        \"\"\"\n",
        "        self.email = email\n",
        "        self.password = password\n",
        "        self.driver = None\n",
        "        self.setup_driver()\n",
        "\n",
        "    def setup_driver(self):\n",
        "        \"\"\"Set up Chrome driver with appropriate options\"\"\"\n",
        "        chrome_options = Options()\n",
        "        chrome_options.add_argument('--headless')  # Run in headless mode\n",
        "        chrome_options.add_argument('--no-sandbox')\n",
        "        chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "        chrome_options.add_argument('--disable-notifications')\n",
        "        chrome_options.add_argument('--disable-gpu')\n",
        "        chrome_options.add_argument('--window-size=1920,1080')\n",
        "\n",
        "        # Add random user agent\n",
        "        user_agents = [\n",
        "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "        ]\n",
        "        chrome_options.add_argument(f'user-agent={random.choice(user_agents)}')\n",
        "\n",
        "        self.driver = webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "    def login(self):\n",
        "        \"\"\"Login to LinkedIn\"\"\"\n",
        "        try:\n",
        "            self.driver.get('https://www.linkedin.com/login')\n",
        "            time.sleep(random.uniform(2, 4))  # Random delay\n",
        "\n",
        "            # Enter email\n",
        "            email_elem = WebDriverWait(self.driver, 10).until(\n",
        "                EC.presence_of_element_located((By.ID, \"username\"))\n",
        "            )\n",
        "            email_elem.send_keys(self.email)\n",
        "\n",
        "            # Enter password\n",
        "            password_elem = self.driver.find_element(By.ID, \"password\")\n",
        "            password_elem.send_keys(self.password)\n",
        "\n",
        "            # Click login button\n",
        "            login_button = self.driver.find_element(By.CSS_SELECTOR, \"[type='submit']\")\n",
        "            login_button.click()\n",
        "\n",
        "            time.sleep(random.uniform(3, 5))  # Wait for login to complete\n",
        "\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Login failed: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "    def get_profile_data(self, profile_url: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Scrape profile data from LinkedIn\n",
        "        Args:\n",
        "            profile_url (str): LinkedIn profile URL\n",
        "        Returns:\n",
        "            Dict[str, Any]: Dictionary containing profile data\n",
        "        \"\"\"\n",
        "        try:\n",
        "            self.driver.get(profile_url)\n",
        "            time.sleep(random.uniform(2, 4))  # Random delay between requests\n",
        "\n",
        "            # Wait for main content to load\n",
        "            WebDriverWait(self.driver, 10).until(\n",
        "                EC.presence_of_element_located((By.CLASS_NAME, \"pv-top-card\"))\n",
        "            )\n",
        "\n",
        "            soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
        "\n",
        "            # Extract basic profile information\n",
        "            profile_data = {\n",
        "                'url': profile_url,\n",
        "                'name': self._safe_extract(soup, \"h1.text-heading-xlarge\"),\n",
        "                'headline': self._safe_extract(soup, \"div.text-body-medium\"),\n",
        "                'company': self._safe_extract(soup, \"span.pv-text-details__right-panel-item-text\"),\n",
        "                'location': self._safe_extract(soup, \"span.text-body-small.inline\"),\n",
        "                'about': self._safe_extract(soup, \"div.pv-shared-text-with-see-more\"),\n",
        "                'followers': self._extract_followers(soup),\n",
        "                'connections': self._extract_connections(soup),\n",
        "                'posts': self._get_recent_posts()\n",
        "            }\n",
        "\n",
        "            return profile_data\n",
        "\n",
        "        except TimeoutException:\n",
        "            print(f\"Timeout while loading profile: {profile_url}\")\n",
        "            return {}\n",
        "        except Exception as e:\n",
        "            print(f\"Error scraping profile {profile_url}: {str(e)}\")\n",
        "            return {}\n",
        "\n",
        "    def _safe_extract(self, soup: BeautifulSoup, selector: str) -> str:\n",
        "        \"\"\"Safely extract text from BeautifulSoup element\"\"\"\n",
        "        try:\n",
        "            element = soup.select_one(selector)\n",
        "            return element.get_text(strip=True) if element else \"\"\n",
        "        except Exception:\n",
        "            return \"\"\n",
        "\n",
        "    def _extract_followers(self, soup: BeautifulSoup) -> int:\n",
        "        \"\"\"Extract number of followers\"\"\"\n",
        "        try:\n",
        "            followers_text = soup.find(text=lambda t: 'followers' in t.lower())\n",
        "            if followers_text:\n",
        "                return int(''.join(filter(str.isdigit, followers_text)))\n",
        "            return 0\n",
        "        except Exception:\n",
        "            return 0\n",
        "\n",
        "    def _extract_connections(self, soup: BeautifulSoup) -> int:\n",
        "        \"\"\"Extract number of connections\"\"\"\n",
        "        try:\n",
        "            connections_text = soup.find(text=lambda t: 'connections' in t.lower())\n",
        "            if connections_text:\n",
        "                return int(''.join(filter(str.isdigit, connections_text)))\n",
        "            return 0\n",
        "        except Exception:\n",
        "            return 0\n",
        "\n",
        "    def _get_recent_posts(self) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Get recent posts data\"\"\"\n",
        "        posts = []\n",
        "        try:\n",
        "            # Click \"Posts\" tab if it exists\n",
        "            posts_tab = WebDriverWait(self.driver, 5).until(\n",
        "                EC.presence_of_element_located((By.CSS_SELECTOR, \"a[href*='recent-activity/shares/']\"))\n",
        "            )\n",
        "            posts_tab.click()\n",
        "            time.sleep(random.uniform(2, 3))\n",
        "\n",
        "            # Get post elements\n",
        "            post_elements = self.driver.find_elements(By.CSS_SELECTOR, \"div.feed-shared-update-v2\")\n",
        "\n",
        "            for post in post_elements[:5]:  # Get last 5 posts\n",
        "                try:\n",
        "                    post_data = {\n",
        "                        'timestamp': self._safe_find_element(post, \"span.feed-shared-actor__sub-description\"),\n",
        "                        'text': self._safe_find_element(post, \"div.feed-shared-text\"),\n",
        "                        'likes': self._extract_reaction_count(post, \"likes\"),\n",
        "                        'comments': self._extract_reaction_count(post, \"comments\")\n",
        "                    }\n",
        "                    posts.append(post_data)\n",
        "                except Exception:\n",
        "                    continue\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error getting posts: {str(e)}\")\n",
        "\n",
        "        return posts\n",
        "\n",
        "    def _safe_find_element(self, element, selector: str) -> str:\n",
        "        \"\"\"Safely find and extract text from element\"\"\"\n",
        "        try:\n",
        "            return element.find_element(By.CSS_SELECTOR, selector).text.strip()\n",
        "        except NoSuchElementException:\n",
        "            return \"\"\n",
        "\n",
        "    def _extract_reaction_count(self, post_element, reaction_type: str) -> int:\n",
        "        \"\"\"Extract reaction count (likes or comments) from post\"\"\"\n",
        "        try:\n",
        "            selector = f\"button.social-details-social-counts__reactions-count\" if reaction_type == \"likes\" else \"button.social-details-social-counts__comments-count\"\n",
        "            element = post_element.find_element(By.CSS_SELECTOR, selector)\n",
        "            count_text = element.text.strip()\n",
        "            return int(''.join(filter(str.isdigit, count_text)))\n",
        "        except Exception:\n",
        "            return 0\n",
        "\n",
        "    def process_profiles(self, profile_urls: List[str]) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Process multiple LinkedIn profiles\n",
        "        Args:\n",
        "            profile_urls (List[str]): List of LinkedIn profile URLs\n",
        "        Returns:\n",
        "            pd.DataFrame: DataFrame containing profile data\n",
        "        \"\"\"\n",
        "        all_data = []\n",
        "\n",
        "        try:\n",
        "            if not self.login():\n",
        "                raise Exception(\"Failed to login to LinkedIn\")\n",
        "\n",
        "            for url in profile_urls:\n",
        "                try:\n",
        "                    print(f\"Processing profile: {url}\")\n",
        "                    profile_data = self.get_profile_data(url)\n",
        "                    if profile_data:\n",
        "                        all_data.append(profile_data)\n",
        "\n",
        "                    # Random delay between profiles\n",
        "                    time.sleep(random.uniform(3, 7))\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing profile {url}: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "        finally:\n",
        "            if self.driver:\n",
        "                self.driver.quit()\n",
        "\n",
        "        return pd.DataFrame(all_data)\n",
        "\n",
        "def analyze_profiles(df: pd.DataFrame) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Analyze profile data\n",
        "    Args:\n",
        "        df (pd.DataFrame): DataFrame containing profile data\n",
        "    Returns:\n",
        "        Dict[str, Any]: Dictionary containing analysis results\n",
        "    \"\"\"\n",
        "    if df.empty:\n",
        "        return {}\n",
        "\n",
        "    analysis = {\n",
        "        'total_profiles': len(df),\n",
        "        'locations': df['location'].value_counts().to_dict(),\n",
        "        'avg_followers': df['followers'].mean(),\n",
        "        'avg_connections': df['connections'].mean(),\n",
        "        'companies': df['company'].value_counts().to_dict()\n",
        "    }\n",
        "\n",
        "    # Analyze posts if available\n",
        "    if 'posts' in df.columns:\n",
        "        post_data = []\n",
        "        for posts in df['posts']:\n",
        "            if posts:\n",
        "                for post in posts:\n",
        "                    post_data.append({\n",
        "                        'likes': post.get('likes', 0),\n",
        "                        'comments': post.get('comments', 0)\n",
        "                    })\n",
        "\n",
        "        if post_data:\n",
        "            post_df = pd.DataFrame(post_data)\n",
        "            analysis.update({\n",
        "                'avg_post_likes': post_df['likes'].mean(),\n",
        "                'avg_post_comments': post_df['comments'].mean(),\n",
        "                'max_post_likes': post_df['likes'].max(),\n",
        "                'max_post_comments': post_df['comments'].max()\n",
        "            })\n",
        "\n",
        "    return analysis\n",
        "\n",
        "def main(urls: List[str], email: str, password: str):\n",
        "    \"\"\"\n",
        "    Main function to run the LinkedIn profile analysis\n",
        "    Args:\n",
        "        urls (List[str]): List of LinkedIn profile URLs\n",
        "        email (str): LinkedIn login email\n",
        "        password (str): LinkedIn login password\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Initialize scraper\n",
        "        scraper = LinkedInScraper(email, password)\n",
        "\n",
        "        # Process profiles\n",
        "        df = scraper.process_profiles(urls)\n",
        "\n",
        "        if df.empty:\n",
        "            print(\"No data was collected. Please check your login credentials and URLs.\")\n",
        "            return\n",
        "\n",
        "        # Analyze data\n",
        "        analysis = analyze_profiles(df)\n",
        "\n",
        "        # Print results\n",
        "        print(\"\\nAnalysis Results:\")\n",
        "        print(f\"Total Profiles Analyzed: {analysis['total_profiles']}\")\n",
        "        print(f\"\\nLocation Distribution:\")\n",
        "        for loc, count in analysis['locations'].items():\n",
        "            print(f\"  {loc}: {count}\")\n",
        "\n",
        "        print(f\"\\nCompany Distribution:\")\n",
        "        for company, count in analysis['companies'].items():\n",
        "            print(f\"  {company}: {count}\")\n",
        "\n",
        "        print(f\"\\nEngagement Metrics:\")\n",
        "        print(f\"  Average Followers: {analysis['avg_followers']:.2f}\")\n",
        "        print(f\"  Average Connections: {analysis['avg_connections']:.2f}\")\n",
        "\n",
        "        if 'avg_post_likes' in analysis:\n",
        "            print(f\"\\nPost Engagement:\")\n",
        "            print(f\"  Average Likes per Post: {analysis['avg_post_likes']:.2f}\")\n",
        "            print(f\"  Average Comments per Post: {analysis['avg_post_comments']:.2f}\")\n",
        "            print(f\"  Highest Likes on a Post: {analysis['max_post_likes']}\")\n",
        "            print(f\"  Highest Comments on a Post: {analysis['max_post_comments']}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in main execution: {str(e)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example usage:\n",
        "    linkedin_urls = [\n",
        "        # Your list of LinkedIn URLs here\n",
        "    ]\n",
        "    linkedin_email = \"your_email@example.com\"\n",
        "    linkedin_password = \"your_password\"\n",
        "\n",
        "    main(linkedin_urls, linkedin_email, linkedin_password)"
      ],
      "metadata": {
        "id": "_IyBvEJ_XvPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qz-rD6e3ovJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AwzeEIODXvRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TSISkoSgXvVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FAHxN8LFWoxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "moft8mrSibRO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM1vpmsu2ZDrM1jmjOO3w8x",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}